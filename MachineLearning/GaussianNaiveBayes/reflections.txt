accuracy = number of correct 'hits' divided by total number of 'hits'

Bayes' Rule
Example (generalized)
P(C) --> percent cancer appears in population (chance individual has cancer) = 0.01
Test = 90% it is positive if you have cancer (this is the sensitivity) (IE it reports correctly that you do have cancer 90% of the time) P(Pos|C)
				sensitivity = number of true positives divided by the sum of the number of true positives plus the number of false negatives
										= number of true positives divided by the number of sick people in population
										= probability of a positive test, given that patient is ill. P(Pos|C)
       90% it is negative if you don't have cancer (this is the specitivity) (IE it reports correctly that you don't have cancer 90% of the time) P(Neg|~C)
				seems to be some conflict between the word specitivity and specificity ... the udacity course uses the former while I belive the later si the correct word
				there are a few other times in medical papers that specitivity is used but in all case I found it appeared to be a misuse/malaproprism of sorts
				specificity = number of false negatives divided by the sum of the number of true negatives plus the number of false positives
										= number of false negatives divided by the total number of healthy people in population
										= probability of a negative test given the patient is healthy. P(Neg|~C)
Bayes Rule
	Prior Probability * Test Result --> Posterior Probability
Prior:
	P(C) = 0.01
		P(~C) = 0.99
		P(Pos|C) = 0.90
		P(Pos|~C) = 0.10
		P(Neg|~C) = 0.90
Posterior:
	Get Joint Probabilities
	P(C,Pos) --> chance an individual has cancer given that they were tested positive
		= P(C) * P(Pos|C)
		= 0.01 * .9 = 0.009
	P(~C,Pos) -->
		= P(~C) * P(Pos|~C)
		= 0.99 * 0.10 = 0.099
Normalizing
		P(Pos) = P(C,Pos) + P(~C,Pos) = 0.108
		P(C|Pos) = P(C,Pos) / P(Pos) = 0.009/0.108 = 0.083333...
		P(~C|Pos) = P(~C,Pos) / P(Pos) = 0.099/0.108 = 0.916666...
		Note: P(C|Pos) + P(~C|Pos) = 1.0 (always)
		
P(C) prior
P(Pos|C) sensitivity
P(~Pos|~C) specificity

Another Example
Prior: P(Chris) = 0.5 --> P(Sarah) = 0.5 (one or the other with 50% chance)
Chris writes with words love, deal, life with frequency (respectively) (0.10,0.80,0.10)
Sarah writes with words love, deal, life with frequency (respectively) (0.50,0.20,0.30)
get message "life deal"
P(Chris|Message)?
P(Sarah|Message)?
 P("life"|Chris)*P("deal"|Chris)*P(Chris) = 0.80*0.10*0.50 = 0.04
 P("life"|Sarah)*P("deal"|Sarah)*P(Sarah) = 0.20*0.30*0.50 = 0.03
 P("life deal") = 0.07
P(Chris|Message) = 0.04/0.07 = 0.57142857142857142857142857142857
P(Sarah|Message) = 0.03/0.07 = 0.42857142857142857142857142857143

It's called naive bayes because it ignores assumptions. E.G. in the above example we ignore the order of the words ... we assume that word order is not meaningful
Naive Bayes is easy to implement and is relatively quick to calculate and is efficient
